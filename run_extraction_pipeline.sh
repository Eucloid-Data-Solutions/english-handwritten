#!/bin/bash

set -e

# Configuration
VENV_PATH="/opt/pytorch/bin/activate"
VLLM_MODEL="/english-handwritten/data/models/gemma-3-12b-it"
VLLM_PORT=8001
AIRFLOW_DAG="document_extraction_v2"  # Change to "document_extraction_parallel" if preferred
LOG_DIR="/english-handwritten/tmp/extraction_logs"

# Create log directory
mkdir -p $LOG_DIR

echo "ğŸš€ Starting Document Extraction Pipeline"
echo "========================================"

# 1. Activate virtual environment
echo "ğŸ”§ Activating virtual environment..."
source $VENV_PATH
echo "âœ… Virtual environment activated"

# 2. Check if vLLM server is running
echo "ğŸ” Checking vLLM server status..."
if curl -s http://localhost:$VLLM_PORT/health > /dev/null 2>&1; then
    echo "âœ… vLLM server is already running on port $VLLM_PORT"
else
    echo "ğŸš€ Starting vLLM server..."
    nohup python -m vllm.entrypoints.openai.api_server \
        --model $VLLM_MODEL \
        --dtype bfloat16 \
        --tensor-parallel-size 4 \
        --host 0.0.0.0 \
        --port $VLLM_PORT \
        > $LOG_DIR/vllm_server.log 2>&1 &
    
    VLLM_PID=$!
    echo "ğŸ“ vLLM server started with PID: $VLLM_PID"
    
    # Wait for server to be ready
    echo "â³ Waiting for vLLM server to be ready..."
    for i in {1..60}; do
        if curl -s http://localhost:$VLLM_PORT/health > /dev/null 2>&1; then
            echo "âœ… vLLM server is ready!"
            break
        fi
        echo "   Attempt $i/60... waiting 10 seconds"
        sleep 10
    done
    
    # Final check
    if ! curl -s http://localhost:$VLLM_PORT/health > /dev/null 2>&1; then
        echo "âŒ vLLM server failed to start. Check logs: $LOG_DIR/vllm_server.log"
        exit 1
    fi
fi

# 3. Test vLLM server with a simple request
echo "ğŸ§ª Testing vLLM server..."
TEST_RESPONSE=$(curl -s -X POST http://localhost:$VLLM_PORT/v1/chat/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "'$VLLM_MODEL'",
        "messages": [{"role": "user", "content": "Hello, respond with just: OK"}],
        "max_tokens": 10
    }' | grep -o '"content":"[^"]*"' | head -1)

if [[ -n "$TEST_RESPONSE" ]]; then
    echo "âœ… vLLM server test successful"
else
    echo "âŒ vLLM server test failed"
    exit 1
fi

# 4. Initialize Airflow
echo "ğŸ”§ Initializing Airflow..."

# Set Airflow home and configuration
export AIRFLOW_HOME="$PWD/airflow"
export AIRFLOW__CORE__LOAD_EXAMPLES=False
export AIRFLOW__CORE__DAGS_FOLDER="$PWD/airflow/dags"

if [[ ! -f "$AIRFLOW_HOME/airflow.db" ]]; then
    echo "ğŸ“Š Initializing Airflow database..."
    airflow db init
else
    echo "âœ… Airflow database already exists"
fi

# Check if DAG file exists
if [[ ! -f "$PWD/airflow/dags/extraction_dag.py" ]]; then
    echo "âŒ DAG file not found at $PWD/airflow/dags/extraction_dag.py"
    exit 1
fi

# Start scheduler and webserver
echo "ğŸ”„ Starting Airflow scheduler..."
nohup airflow scheduler > $LOG_DIR/scheduler.log 2>&1 &

echo "ğŸŒ Starting Airflow webserver..."
nohup airflow webserver --port 8080 > $LOG_DIR/webserver.log 2>&1 &

# Wait for services to start
sleep 15

# Parse DAGs to ensure they're loaded
echo "ğŸ“‹ Parsing DAGs..."
airflow dags reserialize

# Start scheduler to process queue
echo "ğŸ”„ Starting Airflow scheduler..."
nohup airflow scheduler > $LOG_DIR/scheduler.log 2>&1 &
SCHEDULER_PID=$!
echo "ğŸ“ Scheduler started with PID: $SCHEDULER_PID"

# Wait for scheduler to start
sleep 10

# List available DAGs to verify
echo "ğŸ“‹ Available DAGs:"
airflow dags list | grep -E "(document_extraction|extraction)" || echo "âš ï¸  No extraction DAGs found"

# 5. Check if database exists
DB_PATH="$HOME/english-handwritten/data/db/extraction.db"
if [[ ! -f "$DB_PATH" ]]; then
    echo "âš ï¸  Database not found. Creating directory..."
    mkdir -p "$HOME/english-handwritten/data/db"
    echo "ğŸ“ Database will be created during first extraction at $DB_PATH"
fi

# 6. Trigger Airflow DAG
echo "ğŸ¯ Triggering Airflow DAG: $AIRFLOW_DAG"
airflow dags trigger $AIRFLOW_DAG

# 7. Monitor DAG execution
echo "ğŸ“Š Monitoring DAG execution..."

# Simple monitoring without JSON parsing
for i in {1..30}; do
    STATE=$(airflow dags state $AIRFLOW_DAG 2>/dev/null | tail -1 | awk '{print $NF}' || echo "unknown")
    echo "   Status check $i/30: $STATE"
    
    case $STATE in
        "success")
            echo "ğŸ‰ DAG execution completed successfully!"
            break
            ;;
        "failed")
            echo "âŒ DAG execution failed. Check Airflow UI for details."
            exit 1
            ;;
        "running"|"queued")
            echo "   DAG is $STATE..."
            sleep 30
            ;;
        *)
            echo "   DAG state: $STATE"
            sleep 30
            ;;
    esac
done

# 8. Show results summary
echo "ğŸ“ˆ Checking extraction results..."
if [[ -f "$DB_PATH" ]]; then
    TOTAL_DOCS=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM documents;" 2>/dev/null || echo "0")
    INDEX1_ENTRIES=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM index1_entries;" 2>/dev/null || echo "0")
    INDEX2_ENTRIES=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM index2_entries;" 2>/dev/null || echo "0")
    
    echo "âœ… Extraction Summary:"
    echo "   ğŸ“„ Total documents processed: $TOTAL_DOCS"
    echo "   ğŸ‘¥ INDEX I entries: $INDEX1_ENTRIES"
    echo "   ğŸ  INDEX II entries: $INDEX2_ENTRIES"
else
    echo "âš ï¸  Database not found - extraction may have failed"
fi

echo ""
echo "ğŸ Pipeline execution completed!"
echo "ğŸ“Š Airflow UI: http://3.83.158.201:8080 (external) or http://localhost:8080 (local)"
echo "ğŸ” vLLM logs: $LOG_DIR/vllm_server.log"
echo "ğŸ“‹ Scheduler logs: $LOG_DIR/scheduler.log"
echo "ğŸŒ Webserver logs: $LOG_DIR/webserver.log"
echo "ğŸ’¾ Database: $DB_PATH"